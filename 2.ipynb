{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载声明数据\n",
    "claim_file_path = \"data/train-claims.json\"\n",
    "with open(claim_file_path, \"r\") as file:\n",
    "    claims_data = json.load(file)\n",
    "\n",
    "# 加载证据数据\n",
    "evidence_file_path = \"data/evidence.json\"\n",
    "with open(evidence_file_path, \"r\") as file:\n",
    "    evidence_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换声明数据为DataFrame\n",
    "claims = []\n",
    "for claim_id, details in claims_data.items():\n",
    "    claims.append({'claim_id': claim_id, **details})\n",
    "claims_df = pd.DataFrame(claims)\n",
    "\n",
    "# 转换证据数据为DataFrame\n",
    "evidences = []\n",
    "for ev_id, text in evidence_data.items():\n",
    "    evidences.append({'evidence_id': ev_id, 'text': text})\n",
    "evidence_df = pd.DataFrame(evidences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evidence-545412'\n",
      " \"The Bureau (original title : Le Bureau des légendes) is a French political thriller television series created by Éric Rochant and produced by Canal +, which revolves around the lives of agents of the DGSE (General Directorate of External Security), France 's principal external security service.\"]\n"
     ]
    }
   ],
   "source": [
    "print(evidence_df[evidence_df['evidence_id'] ==  'evidence-545412'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载Spacy英语模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 创建NER映射表并进行实体提取\n",
    "ner_map = defaultdict(list)\n",
    "\n",
    "for index, row in evidence_df.iterrows():\n",
    "    doc = nlp(row['text'])\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent.text)\n",
    "    ner_map[row['evidence_id']] = entities  # 保存每个证据的实体列表\n",
    "\n",
    "# 将实体信息添加到DataFrame\n",
    "evidence_df['entities'] = evidence_df['evidence_id'].map(ner_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text, entities):\n",
    "    \"\"\"保留英文单词和特定模式（如化学符号、数字与字母的组合等），同时保留已识别的实体\"\"\"\n",
    "    pattern = re.compile(r'\\b[a-zA-Z0-9]+\\b')\n",
    "    tokens = word_tokenize(text)\n",
    "    entities = set(entities)  # 将实体列表转换为集合以快速检查\n",
    "    filtered_tokens = [token for token in tokens if pattern.match(token) or token in entities]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# 应用文本过滤，同时考虑实体\n",
    "claims_df['claim_text'] = claims_df.apply(lambda row: filter_text(row['claim_text'], row.get('entities', [])), axis=1)\n",
    "evidence_df['text'] = evidence_df.apply(lambda row: filter_text(row['text'], row['entities']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_entities(text, entities):\n",
    "    \"\"\"使用Spacy进行分词，并对实体进行加权处理\"\"\"\n",
    "    doc = nlp(text)\n",
    "    words = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    # 增加实体的出现次数\n",
    "    words.extend([entity.lower() for entity in entities for _ in range(3)])  # 实体权重增加，出现3次\n",
    "    return words\n",
    "\n",
    "# 应用分词并考虑实体\n",
    "evidence_df['tokens'] = evidence_df.apply(lambda row: tokenize_with_entities(row['text'], row['entities']), axis=1)\n",
    "bm25 = BM25Okapi(evidence_df['tokens'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    \"\"\"保留英文单词和特定模式（如化学符号、数字与字母的组合等）\"\"\"\n",
    "    pattern = re.compile(r'\\b[a-zA-Z0-9]+\\b')  # 识别字母和数字的组合\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if pattern.match(token)]\n",
    "    return \" \".join(filtered_tokens)\n",
    "# 应用文本过滤函数到正确的列\n",
    "dev_claims_df['claim_text'] = dev_claims_df['claim_text'].apply(filter_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载开发集数据\n",
    "dev_claim_file_path = \"data/dev-claims.json\"\n",
    "with open(dev_claim_file_path, \"r\") as file:\n",
    "    dev_claims_data = json.load(file)\n",
    "\n",
    "dev_claims = []\n",
    "for claim_id, details in dev_claims_data.items():\n",
    "    dev_claims.append({'claim_id': claim_id, **details})\n",
    "dev_claims_df = pd.DataFrame(dev_claims)\n",
    "\n",
    "# 应用文本过滤函数到正确的列\n",
    "dev_claims_df['claim_text'] = dev_claims_df['claim_text'].apply(filter_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2973523421588595\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_evidence(claim_text, entities, top_n=20):\n",
    "    # 为声明文本和相关实体构建查询令牌\n",
    "    claim_tokens = tokenize_with_entities(claim_text, entities)\n",
    "    scores = bm25.get_scores(claim_tokens)\n",
    "    top_indexes = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "    return [evidence_df.iloc[i]['evidence_id'] for i in top_indexes]\n",
    "\n",
    "# 检索证据并计算准确率\n",
    "correct_hits = 0\n",
    "total_evidences = 0\n",
    "\n",
    "for index, row in dev_claims_df.iterrows():\n",
    "    predicted_evidences = get_top_n_evidence(row['claim_text'], row.get('entities', []))\n",
    "    actual_evidences = row['evidences']\n",
    "    correct_hits += len(set(predicted_evidences) & set(actual_evidences))\n",
    "    total_evidences += len(actual_evidences)\n",
    "\n",
    "accuracy = correct_hits / total_evidences\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3890020366598778\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_evidence(claim_text, entities, top_n=40):\n",
    "    claim_tokens = tokenize_with_entities(claim_text, entities)\n",
    "    scores = bm25.get_scores(claim_tokens)\n",
    "    top_indexes = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "    return [evidence_df.iloc[i]['evidence_id'] for i in top_indexes]\n",
    "predicted_results = {}\n",
    "correct_hits = 0\n",
    "total_evidences = 0\n",
    "\n",
    "for index, row in dev_claims_df.iterrows():\n",
    "    predicted_evidences = get_top_n_evidence(row['claim_text'], row.get('entities', []))\n",
    "    actual_evidences = row['evidences']\n",
    "    correct_hits += len(set(predicted_evidences) & set(actual_evidences))\n",
    "    total_evidences += len(actual_evidences)\n",
    "    predicted_results[row['claim_id']] = {\n",
    "        \"claim_text\": row['claim_text'],\n",
    "        \"evidences\": predicted_evidences\n",
    "    }\n",
    "\n",
    "accuracy = correct_hits / total_evidences\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "import json\n",
    "\n",
    "# 保存预测结果到JSON文件\n",
    "with open('data/predicted_evidences_dev.json', 'w') as f:\n",
    "    json.dump(predicted_results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42973523421588594\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_evidence(claim_text, entities, top_n=60):\n",
    "    claim_tokens = tokenize_with_entities(claim_text, entities)\n",
    "    scores = bm25.get_scores(claim_tokens)\n",
    "    top_indexes = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "    return [evidence_df.iloc[i]['evidence_id'] for i in top_indexes]\n",
    "predicted_results = {}\n",
    "correct_hits = 0\n",
    "total_evidences = 0\n",
    "\n",
    "for index, row in dev_claims_df.iterrows():\n",
    "    predicted_evidences = get_top_n_evidence(row['claim_text'], row.get('entities', []))\n",
    "    actual_evidences = row['evidences']\n",
    "    correct_hits += len(set(predicted_evidences) & set(actual_evidences))\n",
    "    total_evidences += len(actual_evidences)\n",
    "    predicted_results[row['claim_id']] = {\n",
    "        \"claim_text\": row['claim_text'],\n",
    "        \"evidences\": predicted_evidences\n",
    "    }\n",
    "\n",
    "accuracy = correct_hits / total_evidences\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "import json\n",
    "\n",
    "# 保存预测结果到JSON文件\n",
    "with open('data/predicted_evidences_dev_60.json', 'w') as f:\n",
    "    json.dump(predicted_results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4623217922606925\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_evidence(claim_text, entities, top_n=80):\n",
    "    claim_tokens = tokenize_with_entities(claim_text, entities)\n",
    "    scores = bm25.get_scores(claim_tokens)\n",
    "    top_indexes = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "    return [evidence_df.iloc[i]['evidence_id'] for i in top_indexes]\n",
    "predicted_results = {}\n",
    "correct_hits = 0\n",
    "total_evidences = 0\n",
    "\n",
    "for index, row in dev_claims_df.iterrows():\n",
    "    predicted_evidences = get_top_n_evidence(row['claim_text'], row.get('entities', []))\n",
    "    actual_evidences = row['evidences']\n",
    "    correct_hits += len(set(predicted_evidences) & set(actual_evidences))\n",
    "    total_evidences += len(actual_evidences)\n",
    "    predicted_results[row['claim_id']] = {\n",
    "        \"claim_text\": row['claim_text'],\n",
    "        \"evidences\": predicted_evidences\n",
    "    }\n",
    "\n",
    "accuracy = correct_hits / total_evidences\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "import json\n",
    "\n",
    "# 保存预测结果到JSON文件\n",
    "with open('data/predicted_evidences_dev_80.json', 'w') as f:\n",
    "    json.dump(predicted_results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4908350305498982\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_evidence(claim_text, entities, top_n=100):\n",
    "    claim_tokens = tokenize_with_entities(claim_text, entities)\n",
    "    scores = bm25.get_scores(claim_tokens)\n",
    "    top_indexes = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "    return [evidence_df.iloc[i]['evidence_id'] for i in top_indexes]\n",
    "predicted_results = {}\n",
    "correct_hits = 0\n",
    "total_evidences = 0\n",
    "\n",
    "for index, row in dev_claims_df.iterrows():\n",
    "    predicted_evidences = get_top_n_evidence(row['claim_text'], row.get('entities', []))\n",
    "    actual_evidences = row['evidences']\n",
    "    correct_hits += len(set(predicted_evidences) & set(actual_evidences))\n",
    "    total_evidences += len(actual_evidences)\n",
    "    predicted_results[row['claim_id']] = {\n",
    "        \"claim_text\": row['claim_text'],\n",
    "        \"evidences\": predicted_evidences\n",
    "    }\n",
    "\n",
    "accuracy = correct_hits / total_evidences\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "import json\n",
    "\n",
    "# 保存预测结果到JSON文件\n",
    "with open('data/predicted_evidences_dev_100.json', 'w') as f:\n",
    "    json.dump(predicted_results, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5193482688391039\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_evidence(claim_text, entities, top_n=150):\n",
    "    claim_tokens = tokenize_with_entities(claim_text, entities)\n",
    "    scores = bm25.get_scores(claim_tokens)\n",
    "    top_indexes = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "    return [evidence_df.iloc[i]['evidence_id'] for i in top_indexes]\n",
    "predicted_results = {}\n",
    "correct_hits = 0\n",
    "total_evidences = 0\n",
    "\n",
    "for index, row in dev_claims_df.iterrows():\n",
    "    predicted_evidences = get_top_n_evidence(row['claim_text'], row.get('entities', []))\n",
    "    actual_evidences = row['evidences']\n",
    "    correct_hits += len(set(predicted_evidences) & set(actual_evidences))\n",
    "    total_evidences += len(actual_evidences)\n",
    "    predicted_results[row['claim_id']] = {\n",
    "        \"claim_text\": row['claim_text'],\n",
    "        \"evidences\": predicted_evidences\n",
    "    }\n",
    "\n",
    "accuracy = correct_hits / total_evidences\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "import json\n",
    "\n",
    "# 保存预测结果到JSON文件\n",
    "with open('data/predicted_evidences_dev_150.json', 'w') as f:\n",
    "    json.dump(predicted_results, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
