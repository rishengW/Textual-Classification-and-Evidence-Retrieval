{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter, OrderedDict\n",
    "from math import sqrt as msqrt\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(1212)\n",
    "\n",
    "pretrain_model_path = \"data/pretrain.model\"\n",
    "evidence_selection_model_path = \"data/selection.model\"\n",
    "evidence_validation_model_path = \"data/validation.model\"\n",
    "preprocessing_result_path='data/preprocessing_word2idx.csv'\n",
    "preprocessing_evidence_path='data/preprocessing_evidences.csv'\n",
    "\n",
    "pad_idx = 0\n",
    "max_len = 100\n",
    "max_pred = 2\n",
    "min_freq = 5\n",
    "\n",
    "d_model = 256  # n_heads * d_k\n",
    "n_heads = 4\n",
    "n_layers = 4\n",
    "p_dropout = .1\n",
    "# BERT propability defined\n",
    "p_mask = .8\n",
    "p_replace = .1\n",
    "p_do_nothing = 1 - p_mask - p_replace\n",
    "\n",
    "p_dev = 0.1\n",
    "batch_size = 128\n",
    "epoch = 80\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "device = \"cpu\"\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, pretrain_model, max_vocab, max_len, d_out):\n",
    "        super(BERT, self).__init__()\n",
    "        self.max_vocab = max_vocab\n",
    "        self.max_len = max_len\n",
    "        self.d_out= d_out\n",
    "        self.embedding = pretrain_model.embedding\n",
    "        self.encoders = pretrain_model.encoders\n",
    "        self.pooler = pretrain_model.pooler\n",
    "        self.embedding.word_emb.weight.requires_grad = False\n",
    "        self.embedding.seg_emb.weight.requires_grad = True\n",
    "        self.embedding.pos_emb.weight.requires_grad = False\n",
    "        for p in self.encoders.parameters():\n",
    "            p.requires_grad = True\n",
    "        self.pooler.fc.weight.requires_grad = True\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_out),\n",
    "            nn.Softmax(dim=1) ,\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens, segments):\n",
    "        output = self.embedding(tokens, segments)\n",
    "        for layer in self.encoders:\n",
    "            output = layer(output, src_key_padding_mask=tokens.data.eq(pad_idx))\n",
    "        output = self.classifier_head(torch.mean(output, dim=1))\n",
    "        return output\n",
    "\n",
    "    def gelu(self, x):\n",
    "        '''\n",
    "        Two way to implements GELU:\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        or\n",
    "        0.5 * x * (1. + torch.erf(torch.sqrt(x, 2))) \n",
    "        '''\n",
    "        return .5 * x * (1. + torch.erf(x / msqrt(2.)))\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, max_vocab, max_len):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.seg_emb = nn.Embedding(2, d_model)\n",
    "        self.word_emb = nn.Embedding(max_vocab, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        '''\n",
    "        x: [batch, seq_len]\n",
    "        '''\n",
    "        word_enc = self.word_emb(x)\n",
    "\n",
    "        # positional embedding\n",
    "        pos = torch.arange(x.shape[1], dtype=torch.long, device=device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)\n",
    "        pos_enc = self.pos_emb(pos)\n",
    "\n",
    "        seg_enc = self.seg_emb(seg)\n",
    "        x = self.norm(word_enc + pos_enc + seg_enc)\n",
    "        return self.dropout(x)\n",
    "        # return: [batch, seq_len, d_model]\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: [batch, d_model] (first place output)\n",
    "        '''\n",
    "        x = self.fc(x)\n",
    "        x = self.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataPreprocessor():\n",
    "    def __init__(self, pretrain_result_path=preprocessing_result_path):\n",
    "        print(f\"Loading preprocessing result word2idx from {pretrain_result_path}\")\n",
    "        with open(pretrain_result_path, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            self.word2idx = {row[0]: int(row[1]) for row in list(reader)}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.special_token_offset = self.word2idx[next(word for word in self.word2idx if not word.startswith(\"<\"))]\n",
    "    \n",
    "    def preprocess(self, X, filter_empty=True):\n",
    "        X = self.tokenize(X)\n",
    "        X = self.lemmatize(X)\n",
    "        X = self.filterWord(X, filter_empty)\n",
    "        return X\n",
    "    \n",
    "    def get_ids(self, X):\n",
    "        if isinstance(X[0], tuple):\n",
    "            return [([self.word2idx.get(word, self.word2idx['<UNK>']) for word in evidence],\n",
    "                  [self.word2idx.get(word, self.word2idx['<UNK>']) for word in claim]) for evidence, claim in X]\n",
    "        elif isinstance(X[0], list):\n",
    "            return [self.word2idx.get(word, self.word2idx['<UNK>']) for evidence in X for word in evidence]\n",
    "        else:\n",
    "            return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in X]\n",
    "\n",
    "    def tokenize(self, X):\n",
    "        if isinstance(X[0], tuple):\n",
    "            return [(word_tokenize(evidence), word_tokenize(claim)) for evidence, claim in X]\n",
    "        elif isinstance(X, list):\n",
    "            return [word_tokenize(evidence) for evidence in X]\n",
    "        else:\n",
    "            return word_tokenize(X)\n",
    "    \n",
    "     \n",
    "    def lemmatize(self, X):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if isinstance(X[0], tuple):\n",
    "            return [([lemmatizer.lemmatize(word) for word in evidence], [lemmatizer.lemmatize(word) for word in claim]) for evidence, claim in X]\n",
    "        elif isinstance(X[0], list):\n",
    "            return [lemmatizer.lemmatize(word) for claim in X for word in claim]\n",
    "        else:\n",
    "            return [lemmatizer.lemmatize(word) for word in X]\n",
    "\n",
    "\n",
    "    def filterWord(self, X, filter_empty=True):\n",
    "        reg = re.compile(r'^[A-Za-z\\-]+$')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        if isinstance(X[0], tuple):\n",
    "            return list(filter(lambda x: not filter_empty or (len(x[0]) > 0 and len(x[1]) > 0), \n",
    "                      [(list(filter(lambda word: (reg.match(word) and word not in stop_words) or word.isdigit(), evidence)),\n",
    "                        list(filter(lambda word: (reg.match(word) and word not in stop_words) or word.isdigit(), claim)))\n",
    "                         for evidence, claim in X]))\n",
    "        elif isinstance(X[0], list):\n",
    "            return list(filter(lambda x: not filter_empty or len(x) > 0, \n",
    "                      [list(filter(lambda word: (reg.match(word) and word not in stop_words) or word.isdigit(), evidence))\n",
    "                         for evidence in X]))\n",
    "        else:\n",
    "            return list(filter(lambda word: (reg.match(word) and word not in stop_words) or word.isdigit(), X))\n",
    "\n",
    "    def make_data(self, X, y):\n",
    "        return [self.__get_one_case(evidence, claim, label) for (evidence, claim), label in zip(X, y)]\n",
    "\n",
    "    def __get_one_case(self, evidence, claim, label):\n",
    "        input_ids = [self.word2idx['<CLS>']] + evidence[:max_len-len(claim)-3] + [self.word2idx['<SEP>']] + claim + [self.word2idx['<SEP>']]\n",
    "        segment_ids = [0] * (1 + min(len(evidence), max_len-len(claim)-3) + 1) + [1] * (1 + len(claim))\n",
    "\n",
    "        # zero padding for tokens\n",
    "        self.__padding(input_ids, max_len - len(input_ids))\n",
    "        self.__padding(segment_ids, max_len - len(segment_ids))\n",
    "\n",
    "        return [input_ids, segment_ids, label]\n",
    "\n",
    "    def __padding(self, ids, n_pads):\n",
    "        return ids.extend([pad_idx for _ in range(n_pads)])\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_ids, segment_ids, labels):\n",
    "        super(TrainDataset, self).__init__()\n",
    "        self.input_ids = input_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.segment_ids[index], self.labels[index]\n",
    "    \n",
    "class TrainEvaluator():\n",
    "    def __init__(self):\n",
    "        self.batch_num = 0\n",
    "        self.cls_total = 0\n",
    "        self.cls_correct = torch.LongTensor([0]).to(device)\n",
    "        self.loss = torch.FloatTensor([0]).to(device)\n",
    "    def eval(self, predicts, labels, loss):\n",
    "        self.cls_total += labels.size(0)\n",
    "        self.batch_num += 1\n",
    "        self.cls_correct += torch.sum(torch.argmax(predicts, dim=1).view(-1) == labels.view(-1))\n",
    "        self.loss += loss\n",
    "    def get_cls_acc(self):\n",
    "        return self.cls_correct.item() / self.cls_total\n",
    "    def get_loss(self):\n",
    "        return self.loss.item() / self.batch_num \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, traindataloader, devdataloader, epoch=epoch, path=evidence_selection_model_path):\n",
    "\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(sum(p.numel() for p in model.parameters()), \n",
    "                                                                        sum(p.numel() for p in model.parameters() if p.requires_grad)))#看看模型的参数\n",
    "    \n",
    "    best_acc = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for n in range(epoch):\n",
    "        print(f'-------------------- epoch {n+1} --------------------')\n",
    "        evaluator = TrainEvaluator()\n",
    "        t_batch = len(traindataloader)\n",
    "        for i, one_batch in enumerate(traindataloader):\n",
    "            input_ids, segment_ids, labels = [ele.to(device) for ele in one_batch]\n",
    "            predict = model(input_ids, segment_ids)\n",
    "            loss = criterion(predict, labels)\n",
    "            loss = (loss.float()).mean()\n",
    "            evaluator.eval(predict, labels, loss)\n",
    "            print('[ Epoch{}: {}/{} ] '.format(n+1, i+1, t_batch), end='\\r')    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch:{n + 1} [Train] loss: {evaluator.get_loss():.6f} \\t cls_acc: {evaluator.get_cls_acc():.3f}')\n",
    "        model.eval()\n",
    "        evaluator = TrainEvaluator()\n",
    "        t_batch = len(devdataloader)\n",
    "        with torch.no_grad():\n",
    "            for i, one_batch in enumerate(devdataloader):\n",
    "                input_ids, segment_ids, labels = [ele.to(device) for ele in one_batch]\n",
    "                predict = model(input_ids, segment_ids)\n",
    "                loss = criterion(predict, labels)\n",
    "                loss = (loss.float()).mean()\n",
    "                evaluator.eval(predict, labels, loss)\n",
    "        model.train()\n",
    "        \n",
    "        cls_acc, loss = evaluator.get_cls_acc(), evaluator.get_loss()\n",
    "        print(f'Epoch:{n + 1} [Dev]   loss: {loss:.6f} \\t cls_acc: {cls_acc:.3f}')\n",
    "        if cls_acc > best_acc:\n",
    "                # 如果validation的結果好于之前所有的結果，就把当下的模型存下來以便后续的预测使用\n",
    "                best_acc = cls_acc\n",
    "                torch.save(model, path)\n",
    "                print(f'saved model with \\t cls_acc: {cls_acc:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessing result word2idx from data/preprocessing_word2idx.csv\n"
     ]
    }
   ],
   "source": [
    "preprocessor = TrainDataPreprocessor(preprocessing_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save_evidences(preprocessor, preprocessing_evidence_path=preprocessing_evidence_path):\n",
    "    with open('../data/evidence.json', 'r') as file:\n",
    "        evidences = list(json.load(file).values())\n",
    "    evidences = preprocessor.preprocess(evidences, filter_empty=False)\n",
    "    evidences = preprocessor.get_ids(evidences)\n",
    "    with open(preprocessing_evidence_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for evidence in evidences:\n",
    "            writer.writerow(evidence)\n",
    "\n",
    "def load_preprocessing_evidences(preprocessing_evidence_path=preprocessing_evidence_path):\n",
    "    print(f\"Loading preprocessing evidences from {preprocessing_evidence_path}\")\n",
    "    with open(preprocessing_evidence_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        return [[int(id) for id in row] for row in reader]\n",
    "\n",
    "def load_evidence_claim_pairs_for_selection(claim_file_path):\n",
    "    evidence_claim_pairs, labels = [], []\n",
    "    with open(claim_file_path, 'r') as claims:\n",
    "        claims = json.load(claims)\n",
    "    preprocessing_evidences = load_preprocessing_evidences()\n",
    "    for claim in claims.values():\n",
    "        claim_text = claim['claim_text']\n",
    "        claim_text = preprocessor.preprocess(claim_text)\n",
    "        claim_ids = preprocessor.get_ids(claim_text)\n",
    "        for i, evidence in enumerate(claim['evidences']):\n",
    "            evidence_claim_pairs.append((preprocessing_evidences[int(evidence[9:])], claim_ids))\n",
    "            labels.append(1)\n",
    "        for _ in range(i):\n",
    "            while True:\n",
    "                random_evidence = random.choice(preprocessing_evidences)\n",
    "                if random_evidence not in claim['evidences']:\n",
    "                    break\n",
    "            evidence_claim_pairs.append((random_evidence, claim_ids))\n",
    "            labels.append(0)\n",
    "    return evidence_claim_pairs, labels\n",
    "\n",
    "def load_evidence_claim_pairs_for_validation(claim_file_path):\n",
    "    evidence_claim_pairs, labels = [], []\n",
    "    with open(claim_file_path, 'r') as claims:\n",
    "        claims = json.load(claims)\n",
    "    preprocessing_evidences = load_preprocessing_evidences()\n",
    "    for claim in claims.values():\n",
    "        if claim['claim_label'] == 'DISPUTED' or claim['claim_label'] == 'NOT_ENOUGH_INFO':\n",
    "            # need optimize !!!!!!!!!!!!!!!!!\n",
    "            continue \n",
    "        claim_text = claim['claim_text']\n",
    "        claim_text = preprocessor.preprocess(claim_text)\n",
    "        claim_ids = preprocessor.get_ids(claim_text)\n",
    "        for i, evidence in enumerate(claim['evidences']):\n",
    "            evidence_claim_pairs.append((preprocessing_evidences[int(evidence[9:])], claim_ids))\n",
    "            if claim['claim_label'] == 'SUPPORTS':\n",
    "                labels.append(1)\n",
    "            elif claim['claim_label'] == 'REFUTES':\n",
    "                labels.append(2)\n",
    "            else:\n",
    "                break\n",
    "        # use evidence selected by the previous step\n",
    "        # potential_evidences = select_evidence(claim_text)\n",
    "        for _ in range(i):\n",
    "            while True:\n",
    "                random_evidence = random.choice(preprocessing_evidences)\n",
    "                if random_evidence not in claim['evidences']:\n",
    "                    break\n",
    "            evidence_claim_pairs.append((random_evidence, claim_ids))\n",
    "            labels.append(0)\n",
    "            \n",
    "    return evidence_claim_pairs, labels\n",
    "\n",
    "def get_data_loader(preprocessor, evidence_claim_pairs, labels):\n",
    "    data = preprocessor.make_data(evidence_claim_pairs, labels)\n",
    "    data = [torch.LongTensor(ele) for ele in zip(*data)]\n",
    "    return DataLoader(TrainDataset(*data), batch_size=batch_size, shuffle=True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rishe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessing evidences from data/preprocessing_evidences.csv\n",
      "Loading preprocessing evidences from data/preprocessing_evidences.csv\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m get_data_loader(preprocessor, \u001b[38;5;241m*\u001b[39mload_evidence_claim_pairs_for_selection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train-claims.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m dev_dataloader \u001b[38;5;241m=\u001b[39m get_data_loader(preprocessor, \u001b[38;5;241m*\u001b[39mload_evidence_claim_pairs_for_selection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/dev-claims.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m BERT(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrain_model_path\u001b[49m\u001b[43m)\u001b[49m, preprocessor\u001b[38;5;241m.\u001b[39mvocab_size, max_len, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\rishe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rishe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1443\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rishe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\rishe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\rishe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\rishe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:266\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32mc:\\Users\\rishe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:250\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    247\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    254\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    255\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "# preprocess_and_save_evidences(preprocessor)\n",
    "train_dataloader = get_data_loader(preprocessor, *load_evidence_claim_pairs_for_selection('data/train-claims.json'))\n",
    "dev_dataloader = get_data_loader(preprocessor, *load_evidence_claim_pairs_for_selection('data/dev-claims.json'))\n",
    "\n",
    "model = BERT(torch.load(pretrain_model_path), preprocessor.vocab_size, max_len, 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "model.to(device)\n",
    "train(model, optimizer, train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_evidence(selection_model, claim_text, preprocessing_evidences, limit=5):\n",
    "    selection_model.eval()\n",
    "    claim_text = preprocessor.preprocess(claim_text)\n",
    "    claim_ids = preprocessor.get_ids(claim_text)\n",
    "    evidence_claim_pairs = [(evidence_ids, claim_ids) for evidence_ids in preprocessing_evidences]\n",
    "    data = preprocessor.make_data(evidence_claim_pairs, [0]*len(evidence_claim_pairs))\n",
    "    data = [torch.LongTensor(ele) for ele in zip(*data)]\n",
    "    data_loader = DataLoader(TrainDataset(*data), batch_size=1024, shuffle=False, num_workers = 8)\n",
    "    result_idx, result_labels, result_prob = [], [], []\n",
    "    with torch.no_grad():\n",
    "        t_batch = len(data_loader)\n",
    "        idx = 0 \n",
    "        for i, one_batch in enumerate(data_loader):\n",
    "            input_ids, segment_ids, _ = [ele.to(device) for ele in one_batch]\n",
    "            predict = selection_model(input_ids, segment_ids)\n",
    "            result_idx.extend(range(idx, idx + len(predict)))\n",
    "            result_labels.extend(torch.argmax(predict, dim=1).tolist())\n",
    "            result_prob.extend(torch.max(predict, dim=1).tolist())\n",
    "            idx += len(predict)\n",
    "            print(f'[ {i+1}/{t_batch} ] ', end='\\r')\n",
    "    final_result_set = sorted([(idx, label, prob) for idx, label, prob in zip(result_idx, result_labels, result_prob)], key=lambda x: x[2], reverse=True)[:limit]\n",
    "    return [(idx, preprocessing_evidences[idx]) for idx, _, _ in final_result_set]\n",
    "\n",
    "def find_evidence_text(ids):\n",
    "    with open('/kaggle/input/train-dataset/evidence.json', 'r') as file:\n",
    "        evidences = json.load(file)\n",
    "        return [evidences[\"evidence-\"+str(id)] for id in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_model = torch.load(evidence_selection_model_path).to(device)\n",
    "preprocessing_evidences = load_preprocessing_evidences(preprocessing_evidence_path)\n",
    "potential_evidence = select_evidence(selection_model, \"[South Australia] has the most expensive electricity in the world.\", preprocessing_evidences)\n",
    "for text in find_evidence_text([id for id, _ in potential_evidence]):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_data_loader(preprocessor, *load_evidence_claim_pairs_for_validation('../data/train-claims.json'))\n",
    "dev_dataloader = get_data_loader(preprocessor, *load_evidence_claim_pairs_for_validation('../data/dev-claims.json'))\n",
    "\n",
    "model = BERT(torch.load(pretrain_model_path), preprocessor.vocab_size, max_len, 3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "model.to(device)\n",
    "train(model, optimizer, train_dataloader, dev_dataloader, path=evidence_validation_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_claim(selection_model, validation_model, claim_text, preprocessing_evidences, limit=5):\n",
    "    validation_model.eval()\n",
    "    potential_evidences = select_evidence(selection_model, claim_text, preprocessing_evidences, limit)\n",
    "    if not potential_evidences:\n",
    "        return 'NOT_ENOUGH_INFO', []\n",
    "\n",
    "    claim_text = preprocessor.preprocess(claim_text)\n",
    "    claim_ids= preprocessor.get_ids(claim_text)\n",
    "    evidence_claim_pairs = [(evidence_ids, claim_ids) for evidence_ids in potential_evidences]\n",
    "    data = preprocessor.make_data(evidence_claim_pairs, [0]*len(evidence_claim_pairs))\n",
    "    data = [torch.LongTensor(ele) for ele in zip(*data)]\n",
    "    data_loader = DataLoader(TrainDataset(*data), batch_size=1024, shuffle=False, num_workers = 8)\n",
    "    result_idx, result_labels, result_prob = [], [], []\n",
    "    with torch.no_grad():\n",
    "        idx = 0\n",
    "        for one_batch in data_loader:\n",
    "            input_ids, segment_ids, _ = [ele.to(device) for ele in one_batch]\n",
    "            predict = validation_model(input_ids, segment_ids)\n",
    "            result_idx.extend(range(idx, idx + len(predict)))\n",
    "            result_labels.extend(torch.argmax(predict, dim=1).tolist())\n",
    "            result_prob.extend(torch.max(predict, dim=1).tolist())\n",
    "            idx += len(predict)\n",
    "    \n",
    "    final_result_set = sorted([(idx, label, prob) for idx, label, prob in zip(result_idx, result_labels, result_prob)], key=lambda x: x[2], reverse=True)[:limit]\n",
    "    if all(label == 0 for _, label, _ in final_result_set):\n",
    "        return 'NOT_ENOUGH_INFO', [potential_evidences[idx] for idx, _, _ in final_result_set]\n",
    "    if any(label == 1 for _, label, _ in final_result_set) and not any(label == 2 for _, label, _ in final_result_set):\n",
    "        return 'SUPPORTS', [potential_evidences[idx] for idx, label, _ in final_result_set if label == 1]\n",
    "    if any(label == 2 for _, label, _ in final_result_set) and not any(label == 1 for _, label, _ in final_result_set):\n",
    "        return 'REFUTES', [potential_evidences[idx] for idx, label, _ in final_result_set if label == 2]\n",
    "    if any(label == 1 for _, label, _ in final_result_set) and any(label == 2 for _, label, _ in final_result_set):\n",
    "        return 'DISPUTED', [potential_evidences[idx] for idx, label, _ in final_result_set if label == 2 or label == 1]\n",
    "    return 'NOT_ENOUGH_INFO', [potential_evidences[idx] for idx, _, _ in final_result_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_model, validation_model = torch.load(evidence_selection_model_path).to(device), torch.load(evidence_validation_model_path).to(device)\n",
    "preprocessing_evidences = load_preprocessing_evidences(preprocessing_evidence_path)\n",
    "label, evidence = predict_single_claim(selection_model, validation_model,\"[South Australia] has the most expensive electricity in the world.\", preprocessing_evidences)\n",
    "print(label)\n",
    "print(find_evidence_text(evidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_claims_and_save(claim_file_path='data/dev-claims.json', output_file_path='data/predict-output.json'):\n",
    "    selection_model, validation_model = torch.load(evidence_selection_model_path), torch.load(evidence_validation_model_path)\n",
    "    preprocessing_evidences = load_preprocessing_evidences(preprocessing_evidence_path, preprocessing_evidences)\n",
    "    with open(claim_file_path, 'r') as claims:\n",
    "        claims = json.load(claims)\n",
    "\n",
    "    result = OrderedDict()   \n",
    "    for claim_id, claim in claims.items():\n",
    "        claim_text = claim['claim_text']\n",
    "        label, evidence = predict_single_claim(selection_model, validation_model, claim_text)\n",
    "        result[claim_id] = {'claim_text': claim_text, 'claim_label': label, 'evidences': evidence}\n",
    "    with open(output_file_path, 'w') as output:\n",
    "            output.write(json.dumps(result))\n",
    "\n",
    "predict_claims_and_save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
